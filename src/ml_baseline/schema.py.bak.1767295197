from __future__ import annotations

from dataclasses import dataclass
from typing import Any

import pandas as pd


@dataclass(frozen=True)
class InputSchema:
    required_feature_columns: list[str]
    optional_id_columns: list[str]
    forbidden_columns: list[str]
    feature_dtypes: dict[str, str]


def _is_numeric_dtype_str(dt: str) -> bool:
    s = (dt or "").lower()
    return any(
        x in s
        for x in ["int", "int64", "int32", "float", "float64", "float32", "number"]
    )


def validate_and_align(
    df_in: pd.DataFrame, schema: InputSchema
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Validate inference/training input against a schema and return:
      - X: DataFrame with REQUIRED features only, aligned in schema order
      - ids: DataFrame with OPTIONAL id columns if present (passthrough)

    Rules:
      - Fail fast if any forbidden columns are present (e.g., target/leakage)
      - Fail fast if any required features are missing (error names the columns)
      - Normalize dtypes: numeric -> to_numeric(errors="coerce"), else -> string dtype
    """
    # 1) Forbidden columns check (fail fast)
    forbidden_present = [c for c in schema.forbidden_columns if c in df_in.columns]
    if forbidden_present:
        raise ValueError(
            f"Forbidden columns present in inference input: {forbidden_present}"
        )

    # 2) Missing required features check (fail fast)
    missing = [c for c in schema.required_feature_columns if c not in df_in.columns]
    if missing:
        raise ValueError(f"Missing required feature columns: {missing}")

    df = df_in.copy()

    # 3) Extract optional IDs if present
    id_cols = [c for c in schema.optional_id_columns if c in df.columns]
    ids = df[id_cols].copy() if id_cols else pd.DataFrame(index=df.index)

    # 4) Normalize dtypes (tolerant: coerce)
    for c, dt in (schema.feature_dtypes or {}).items():
        if c not in df.columns:
            continue
        if _is_numeric_dtype_str(dt):
            df[c] = pd.to_numeric(df[c], errors="coerce")
        else:
            # Use pandas "string" dtype to avoid mixed object issues
            df[c] = df[c].astype("string")

    # 5) Align X to required features only in the exact schema order
    X = df[schema.required_feature_columns].copy()
    return X, ids


def schema_from_dict(d: dict[str, Any]) -> InputSchema:
    # Helper (optional): supports slightly different keys
    return InputSchema(
        required_feature_columns=d.get("required_feature_columns")
        or d.get("required_columns")
        or [],
        optional_id_columns=d.get("optional_id_columns") or d.get("id_columns") or [],
        forbidden_columns=d.get("forbidden_columns") or d.get("forbidden") or [],
        feature_dtypes=d.get("feature_dtypes") or d.get("dtypes") or {},
    )


# === AUTO-ADDED: InputSchema.from_env (bootcamp fix) ===
# This factory is used by train.py to build an InputSchema from a pandas DataFrame.

try:
    import pandas as _pd
except Exception:
    _pd = None

def _dtype_to_str(dtype) -> str:
    """Normalize pandas/numpy dtype to a simple string."""
    if _pd is not None:
        try:
            if _pd.api.types.is_bool_dtype(dtype):   return "bool"
            if _pd.api.types.is_integer_dtype(dtype): return "int"
            if _pd.api.types.is_float_dtype(dtype):   return "float"
        except Exception:
            pass
    s = str(dtype)
    if "int" in s:   return "int"
    if "float" in s: return "float"
    if "bool" in s:  return "bool"
    return "string"

def _auto_build_schema_candidates(feature_names, dtypes_map, id_cols):
    """Return multiple dict shapes to try with schema_from_dict, if available."""
    cols_list = [{"name": n, "dtype": dtypes_map[n], "role": ("id" if n in id_cols else "feature")} for n in feature_names]
    cols_map  = {n: dtypes_map[n] for n in feature_names}

    return [
        {"version": 1, "id_cols": list(id_cols), "columns": cols_list},
        {"version": 1, "id_cols": list(id_cols), "columns": cols_map},
        {"id_cols": list(id_cols), "columns": cols_list},
        {"id_cols": list(id_cols), "columns": cols_map},
        {"id_cols": list(id_cols), "dtypes": cols_map},
        {"schema": {"version": 1, "id_cols": list(id_cols), "columns": cols_list}},
        {"schema": {"version": 1, "id_cols": list(id_cols), "columns": cols_map}},
    ]

def _auto_from_env(cls, df, id_cols=None, target=None, drop_cols=None, **kwargs):
    """
    Build an InputSchema from a DataFrame.
    - Drops the target column (if provided OR if common target names exist).
    - Keeps id_cols as passthrough/ids if present.
    """
    id_cols = list(id_cols or [])
    drop = set(drop_cols or [])
    # If target provided (future-proof)
    if target:
        drop.add(str(target))

    # Common target names (including your assignment target)
    for t in ("is_high_value", "target", "label", "y"):
        if t in getattr(df, "columns", []):
            drop.add(t)

    feature_names = [c for c in df.columns if c not in drop]
    dtypes_map = {c: _dtype_to_str(df[c].dtype) for c in feature_names}

    # 1) If schema_from_dict exists, try multiple shapes until one works
    _sfd = globals().get("schema_from_dict")
    if callable(_sfd):
        for cand in _auto_build_schema_candidates(feature_names, dtypes_map, id_cols):
            try:
                return _sfd(cand)
            except Exception:
                pass

    # 2) Otherwise instantiate InputSchema directly using signature heuristics
    import inspect
    sig = inspect.signature(cls)
    params = [p for p in sig.parameters.values() if p.name != "self" and p.kind not in (p.VAR_POSITIONAL, p.VAR_KEYWORD)]

    def try_init(payload: dict):
        try:
            return cls(**payload)
        except Exception:
            return None

    # Build payload based on constructor parameter names
    names = {p.name for p in params}

    # Try a few common constructor styles
    attempts = []

    # Style A: columns mapping
    if "columns" in names:
        attempts.append({"columns": {c: dtypes_map[c] for c in feature_names}})
        attempts.append({"columns": [{"name": c, "dtype": dtypes_map[c]} for c in feature_names]})

    # Style B: dtypes mapping
    if "dtypes" in names:
        attempts.append({"dtypes": {c: dtypes_map[c] for c in feature_names}})

    # IDs / required / optional / version fields (if exist)
    base = {}
    if "id_cols" in names: base["id_cols"] = list(id_cols)
    if "required" in names: base["required"] = list(feature_names)
    if "optional" in names: base["optional"] = []
    if "version" in names: base["version"] = 1
    if "schema_version" in names: base["schema_version"] = 1
    if "features" in names: base["features"] = list(feature_names)
    if "feature_names" in names: base["feature_names"] = list(feature_names)

    attempts = [{**a, **base} for a in attempts] or [base]

    # Fill any still-required params with something safe
    required = [p.name for p in params if p.default is p.empty]
    for a in attempts:
        for r in required:
            if r not in a:
                # last resort defaults
                a[r] = None

        inst = try_init(a)
        if inst is not None:
            return inst

    # If we got here, we couldn't build it
    raise RuntimeError("AUTO from_env could not instantiate InputSchema; check schema.py constructor fields.")

# Attach method if missing
if not hasattr(InputSchema, "from_env"):
    InputSchema.from_env = classmethod(_auto_from_env)

# === END AUTO-ADDED ===
